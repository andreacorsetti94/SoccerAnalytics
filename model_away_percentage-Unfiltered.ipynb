{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Away Percentage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this module, we are going to create a Machine Learning module that will:\n",
    "1) parse a csv file into a pandas Dataframe\n",
    "2) drop unnecessary columns\n",
    "3) shuffle the Dataframe\n",
    "4) extract the input columns and the target columns\n",
    "5) scaling (preprocessing) the inputs, so that they get standardized\n",
    "6) split the dataset in three samples: the training data (80%), the validation data (10%) and the testing data (10%)\n",
    "7) from each split dataset, we are going to identify their inputs and targets\n",
    "8) from these split dataset we are going to create numpy objects, so that they can be fed into the ML model\n",
    "9) create a 5 hidden layer model\n",
    "10) Run and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_matches = pd.read_csv('V_MATCH_DATA_AND_ODDS.csv', sep=';')\n",
    "\n",
    "filtered_matches.drop('match_id', axis=1, inplace=True)\n",
    "filtered_matches.drop('avg_draw_chance', axis=1, inplace=True)\n",
    "filtered_matches.drop('avg_home_chance', axis=1, inplace=True)\n",
    "\n",
    "match_data = shuffle(filtered_matches)\n",
    "\n",
    "inputs = match_data.iloc[:,:-1]\n",
    "targets = match_data.iloc[:,[-1]]\n",
    "\n",
    "match_data.iloc[:,:-1] = preprocessing.scale(inputs)\n",
    "\n",
    "\n",
    "train_data, validation_data, test_data = np.split(match_data, [int(.8 * len(match_data)), int(.9 * len(match_data))])\n",
    "\n",
    "train_data_inputs, train_data_targets = train_data.iloc[:,:-1], train_data.iloc[:,[-1]]\n",
    "validation_data_inputs, validation_data_targets = validation_data.iloc[:,:-1], validation_data.iloc[:,[-1]]\n",
    "test_data_inputs, test_data_targets = test_data.iloc[:,:-1], test_data.iloc[:,[-1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_data = train_data.to_numpy()\n",
    "np_val_inputs = validation_data_inputs.to_numpy()\n",
    "np_val_targets = validation_data_targets.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##56 inputs\n",
    "#1 output\n",
    "input_size = 56\n",
    "output_size = 1\n",
    "\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = Sequential([\n",
    "            Dense(hidden_layer_size, input_shape=(56,)),\n",
    "            \n",
    "            Dense(hidden_layer_size, activation='tanh'), # 1st hidden layer\n",
    "            Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "            Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "            Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n",
    "            Dense(hidden_layer_size, activation='tanh'), # 5th hidden layer\n",
    "    \n",
    "            Dense(output_size, activation='sigmoid') # output layer\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 17834 samples, validate on 2229 samples\n",
      "Epoch 1/10\n",
      "17834/17834 - 7s - loss: 0.0618 - mean_absolute_error: 0.0618 - val_loss: 0.0582 - val_mean_absolute_error: 0.0582\n",
      "Epoch 2/10\n",
      "17834/17834 - 7s - loss: 0.0562 - mean_absolute_error: 0.0562 - val_loss: 0.0547 - val_mean_absolute_error: 0.0547\n",
      "Epoch 3/10\n",
      "17834/17834 - 10s - loss: 0.0551 - mean_absolute_error: 0.0551 - val_loss: 0.0551 - val_mean_absolute_error: 0.0551\n",
      "Epoch 4/10\n",
      "17834/17834 - 7s - loss: 0.0544 - mean_absolute_error: 0.0544 - val_loss: 0.0545 - val_mean_absolute_error: 0.0545\n",
      "Epoch 5/10\n",
      "17834/17834 - 7s - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0547 - val_mean_absolute_error: 0.0547\n",
      "Epoch 6/10\n",
      "17834/17834 - 7s - loss: 0.0535 - mean_absolute_error: 0.0535 - val_loss: 0.0557 - val_mean_absolute_error: 0.0557\n",
      "Epoch 7/10\n",
      "17834/17834 - 7s - loss: 0.0535 - mean_absolute_error: 0.0535 - val_loss: 0.0547 - val_mean_absolute_error: 0.0547\n",
      "Epoch 8/10\n",
      "17834/17834 - 8s - loss: 0.0531 - mean_absolute_error: 0.0531 - val_loss: 0.0544 - val_mean_absolute_error: 0.0544\n",
      "Epoch 9/10\n",
      "17834/17834 - 8s - loss: 0.0530 - mean_absolute_error: 0.0530 - val_loss: 0.0527 - val_mean_absolute_error: 0.0527\n",
      "Epoch 10/10\n",
      "17834/17834 - 7s - loss: 0.0525 - mean_absolute_error: 0.0525 - val_loss: 0.0542 - val_mean_absolute_error: 0.0542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13886a08b08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_data_inputs, y=train_data_targets, epochs=NUM_EPOCHS, validation_data=(np_val_inputs, np_val_targets), \n",
    "          validation_steps=len(np_val_inputs), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[ 0.12415186 -0.10059033  0.17459673 ...  0.23551574 -0.10637433\n",
      "  -0.08660821]\n",
      " [-0.04406554  0.09460229  0.00806717 ...  0.16606538 -0.16979578\n",
      "  -0.13516305]\n",
      " [ 0.07165766 -0.10130985 -0.09929176 ... -0.02688134 -0.07924639\n",
      "  -0.10402209]\n",
      " ...\n",
      " [ 0.11406249  0.13629973  0.00190357 ... -0.00750989 -0.04181852\n",
      "   0.16349515]\n",
      " [ 0.29017445  0.18121815  0.25181982 ... -0.25391293 -0.15847446\n",
      "  -0.3485326 ]\n",
      " [ 0.16480426  0.07096547  0.08549314 ...  0.02736171  0.15357596\n",
      "  -0.23183767]] \n",
      "Bias: [-0.1749386  -0.02969323 -0.10143669  0.06779473 -0.01896543 -0.06016402\n",
      "  0.19284761  0.16590321  0.02934716 -0.12643662 -0.10541745 -0.17237599\n",
      "  0.04890103 -0.24790168  0.24032857  0.05177315 -0.13648981  0.14722107\n",
      "  0.16472714  0.16779825 -0.00572857 -0.09639323  0.09646895  0.01811767\n",
      " -0.0775419  -0.00554851  0.014658    0.11623517 -0.04757046 -0.11560573\n",
      "  0.11102628  0.03774563 -0.04361786  0.00364473 -0.19528031 -0.15158932\n",
      "  0.01798431 -0.07920744 -0.01173909 -0.01565387 -0.15857586 -0.00508261\n",
      " -0.28278488  0.07037127  0.0319384   0.02370502 -0.00655698 -0.00759442\n",
      " -0.10026776  0.00703736]\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "bias = model.layers[0].get_weights()[1]\n",
    "print('Weights:',weights,'\\nBias:',bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "2230/2230 - 0s - loss: 0.0535 - mean_absolute_error: 0.0535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05353992861014845, 0.053539917]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_data_inputs, y=test_data_targets, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
